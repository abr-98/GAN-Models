{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DCGAN_base_approach.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjbbj0LIijA"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f7xSou4ugTP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eFfjBH2Imon"
      },
      "source": [
        "### Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70xd84a7zJpS"
      },
      "source": [
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import BatchNormalization,Dense,Conv2D,Reshape,Conv2DTranspose,ReLU,LeakyReLU,Flatten,Activation,Input\n",
        "from tensorflow.keras.optimizers import  Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbiNyFv9yTqa"
      },
      "source": [
        "class DCGAN:\n",
        "  def __init__(self,training_images):\n",
        "\n",
        "    ### Considering channels last\n",
        "\n",
        "    self.no_of_samples=training_images.shape[0]\n",
        "    self.height=training_images.shape[1]\n",
        "    self.width=training_images.shape[2]\n",
        "    self.channels=training_images.shape[3]\n",
        "    self.train_data=(training_images-127.5)/127.5 ### Converting grey scale [0-255] -> [-1,1]\n",
        "    self.shape=(self.height,self.width,self.channels)\n",
        "    self.noise_size=100\n",
        "    self.Discriminator=None\n",
        "    self.Generator=None\n",
        "    self.GAN_model=None\n",
        "    self.disc_optimizer=Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "    self.gan_optimizer=Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "    self.loss=BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "\n",
        "  def get_generator(self):\n",
        "\n",
        "    ### Generator definition\n",
        "\n",
        "    w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    Generator= Sequential()\n",
        "\n",
        "    ### Input layer: takes in input a random noise of 100 points distributed in a random distribution \n",
        "    Generator.add(Dense(int(self.height/4)*int(self.width/4)*256,use_bias=False, input_shape=(self.noise_size,),kernel_initializer=w_init))\n",
        "    Generator.add(BatchNormalization())\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Reshaping layer to reshape in image dimension\n",
        "    Generator.add(Reshape((int(self.height/4),int(self.width/4),256)))\n",
        "\n",
        "    ### Upconv layer 1  ## The size remains constant (7 x 7 x 128)\n",
        "    Generator.add(Conv2DTranspose(128, (5,5), strides=(1,1),padding=\"same\",kernel_initializer=w_init,use_bias=False))\n",
        "    Generator.add(BatchNormalization())\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Upconv layer 2  ## The size upsamples by 2 (14 x 14 x 128)\n",
        "    Generator.add(Conv2DTranspose(64, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Generator.add(BatchNormalization())\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Upconv layer 3  ## The size upsamples by 2 (28 x 28 x 1)\n",
        "    Generator.add(Conv2DTranspose(self.channels, (5,5), strides=(2,2),padding=\"same\",kernel_initializer=w_init,activation=\"tanh\"))\n",
        "\n",
        "    ### generator output must be of the dimension of the input shape of the image data: (28,28,1)\n",
        "    ### The last layer uses tanh activation in the generator. \n",
        "    return Generator\n",
        "\n",
        "  def get_discriminator(self):    \n",
        "    \n",
        "    ### Discriminator definition\n",
        "    w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    Discriminator= Sequential()\n",
        "\n",
        "    ### Input layer: Conv layer 1\n",
        "    Discriminator.add(Conv2D(64, (5,5), strides=(2,2),input_shape=self.shape,padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Discriminator.add(BatchNormalization())\n",
        "    Discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    ### Conv layer 2\n",
        "    Discriminator.add(Conv2D(128, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Discriminator.add(BatchNormalization())\n",
        "    Discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    ### Flatten\n",
        "    Discriminator.add(Flatten())\n",
        "\n",
        "    ### Prediction layer\n",
        "    Discriminator.add(Dense(1,kernel_initializer=w_init))\n",
        "\n",
        "    # Discriminator compilation\n",
        "\n",
        "    return Discriminator\n",
        "\n",
        "  def get_GAN(self):  \n",
        "\n",
        "    ### GAN models is made by combining the Generator and Discriminator models\n",
        "\n",
        "    ### Initialization \n",
        "    Input_Tensor=Input((self.noise_size,))\n",
        "\n",
        "    ### Adding generator\n",
        "  \n",
        "    Generated_images=self.Generator(Input_Tensor)\n",
        "\n",
        "    ### Adding Discriminator\n",
        "    Output=self.Discriminator(Generated_images)\n",
        "\n",
        "    GAN_model=Model(inputs=[Input_Tensor],outputs=[Output])\n",
        "\n",
        "    return GAN_model\n",
        "\n",
        "  def get_all_models(self):\n",
        "\n",
        "    self.Discriminator=self.get_discriminator()\n",
        "\n",
        "    self.Generator=self.get_generator()\n",
        "\n",
        "    self.GAN_model=self.get_GAN()\n",
        "\n",
        "    ### Compiling discriminator\n",
        "\n",
        "    print(\"------------ DISCRIMINATOR------------------\")\n",
        "    print(self.Discriminator.summary())\n",
        "    print(\"------------ GENERATOR------------------\")\n",
        "    print(self.Generator.summary())\n",
        "    print(\"------------ GAN------------------\")\n",
        "    print(self.GAN_model.summary())\n",
        "    \n",
        "    return None\n",
        "\n",
        "  def train_on_batch_disc(self,data,labels):\n",
        "    \n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      output=self.Discriminator(data)\n",
        "      loss_disc=self.loss(labels,output)\n",
        "    \n",
        "    d_grads=disc_tape.gradient(loss_disc,self.Discriminator.trainable_variables)\n",
        "    self.disc_optimizer.apply_gradients(zip(d_grads,self.Discriminator.trainable_variables))\n",
        "\n",
        "    pred_prob=tf.round(tf.nn.sigmoid(self.Discriminator(data,training=False)))\n",
        "    disc_acc=tf.reduce_mean(tf.cast(tf.equal(labels,pred_prob),dtype=tf.float32))\n",
        "    acc=disc_acc.numpy()\n",
        "    return loss_disc,acc\n",
        "\n",
        "\n",
        "  def train_on_batch_gan(self,data,labels):\n",
        "    \n",
        "    with tf.GradientTape() as gan_tape:\n",
        "      output=self.GAN_model(data)\n",
        "      loss_gan=self.loss(labels,output)\n",
        "    \n",
        "    g_grads=gan_tape.gradient(loss_gan,self.Generator.trainable_variables)\n",
        "    self.gan_optimizer.apply_gradients(zip(g_grads,self.Generator.trainable_variables))\n",
        "\n",
        "    pred_prob=tf.round(tf.nn.sigmoid(self.GAN_model(data,training=False)))\n",
        "    gan_acc=tf.reduce_mean(tf.cast(tf.equal(labels,pred_prob),dtype=tf.float32))\n",
        "    acc=gan_acc.numpy()\n",
        "    return loss_gan,acc\n",
        "  \n",
        "\n",
        "\n",
        "  def show_images(self, rows=4, columns=4):\n",
        "\n",
        "    z = tf.random.uniform([rows*columns,self.noise_size])\n",
        "\n",
        "    generated_images = self.Generator.predict(z)\n",
        "\n",
        "    generated_images= (generated_images - (-1))/(1 - (-1))\n",
        "\n",
        "    ### Min-Max scaling to convert pixles from [-1,1] -> [0,1]\n",
        "\n",
        "    ### Plotting\n",
        "    fig, ax = plt.subplots(rows,columns, figsize=(6,6))\n",
        "\n",
        "    for i in range(rows):\n",
        "      for j in range(columns):\n",
        "        ax[i,j].imshow(generated_images[i+j,:,:,0],cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "  \n",
        "  \n",
        "  def train(self,epochs,batch_size):\n",
        "\n",
        "    self.get_all_models()  ### Initializing all models\n",
        "\n",
        "    dataset=tf.data.Dataset.from_tensor_slices(self.train_data).shuffle(self.no_of_samples).batch(batch_size,\n",
        "                                                                                                 drop_remainder=True)\n",
        "    #### Creating random batches using tensorflow pipeline\n",
        "    gen_loss=[]\n",
        "    dis_loss=[]\n",
        "\n",
        "    dis_accuracy=[]\n",
        "    gen_accuracy=[]\n",
        "    \n",
        "    history={}\n",
        "\n",
        "    no_of_batches=self.no_of_samples/batch_size\n",
        "\n",
        "    real_labels=np.ones((batch_size,1)) ### For the real images, the label: 1\n",
        "    ### The discriminator must predict 1 for the real images\n",
        "\n",
        "    fake_labels=np.zeros((batch_size,1))### For the fake images, generated by the generator, the label: 0\n",
        "    ### The discriminator must predict 0 for the fake images\n",
        "\n",
        "    for epoch in range(epochs):      ### Training epochs\n",
        "      d_loss=0\n",
        "      g_loss=0\n",
        "      d_acc=0\n",
        "      g_acc=0\n",
        "\n",
        "      for samples in dataset:\n",
        "\n",
        "        ########    DISCRIMINATOR TRAINING    #########\n",
        "\n",
        "        indexes= np.random.randint(0,self.no_of_samples, batch_size)\n",
        "\n",
        "        ### Indexes have the random indexes betweeen 0 and number of samples in the training data. The number of generated\n",
        "        ### indexes is equal to the batch size\n",
        "\n",
        "        samples= self.train_data[indexes]   ### Creating batch\n",
        "\n",
        "        z = tf.random.uniform([batch_size,self.noise_size]) ### Produces batch_size number of 100 dimensional noise arrays of uniform distribution\n",
        "        \n",
        "        generated=self.Generator.predict(z)  ### obtaining the generated images\n",
        "\n",
        "        training_set_of_discriminator=np.concatenate((samples,generated))\n",
        "        labels_of_discriminator=np.concatenate((real_labels,fake_labels))\n",
        "\n",
        "        ### Records discriminator loss for the real images. So, the target labels are all 1s\n",
        "        ### Records discriminator loss for the generated fake images. So, the target labels are all 0s\n",
        "\n",
        "\n",
        "        #### Train_on batch: Scalar training loss (if the model has a single output and no metrics) or list of scalars \n",
        "        #### (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
        "\n",
        "        loss_disc,acc_disc=self.train_on_batch_disc(training_set_of_discriminator,labels_of_discriminator)\n",
        "\n",
        "        #### Train_on batch: Scalar training loss (if the model has a single output and no metrics) or list of scalars \n",
        "        #### (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
        "\n",
        "\n",
        "        ########    GENERATOR TRAINING    #########\n",
        "\n",
        "        #self.Discriminator.trainable=False ### For Generator training the discriminator is not trained\n",
        "\n",
        "        loss_generator,acc_generator=self.train_on_batch_gan(z,real_labels) ### Discriminator is fixed, so the loss is actually genertors\n",
        "      \n",
        "\n",
        "        ### The generator creates images from the random noise, the GAN  model has generator layer added on top of the \n",
        "        ### Discriminator model, so, it's input is the noise for the generator.\n",
        "\n",
        "        ### The generator produces the generated images, which are passed to the discriminator. The discriminator\n",
        "        ### predicts the labels for the images, so the label is the target value\n",
        "\n",
        "        ### The generator wants the discriminator to predict all its objects with label 1. So, the actual labels are passed 1.\n",
        "        ### The error of the generator high if the discriminator distinguishes the image as a fake and produce 0 as prediction.\n",
        "        d_loss+=loss_disc             ## Adding for every batch\n",
        "        g_loss+=loss_generator\n",
        "        d_acc+=acc_disc\n",
        "        g_acc+=acc_generator\n",
        "      \n",
        "      d_loss/=no_of_batches\n",
        "      g_loss/=no_of_batches               #### Getting the mean\n",
        "      d_acc/=no_of_batches\n",
        "      g_acc/=no_of_batches\n",
        "\n",
        "      dis_loss.append(loss_disc)          ### maintaining after each batch\n",
        "      dis_accuracy.append(acc_disc)\n",
        "\n",
        "      gen_loss.append(loss_generator)\n",
        "      gen_accuracy.append(acc_generator)\n",
        "\n",
        "      dis_loss.append(d_loss)\n",
        "      dis_accuracy.append(d_acc)\n",
        "\n",
        "      gen_loss.append(g_loss)\n",
        "      gen_accuracy.append(g_acc)\n",
        "\n",
        "      if epoch%1==0:\n",
        "        #self.show_images()\n",
        "\n",
        "        print(f\"ON EPOCH {epoch} Discriminator Loss: {d_loss}, Discriminator accuracy: {d_acc}, Generator loss: {g_loss}, GAN accuracy: {g_acc}\")\n",
        "  \n",
        "    history[\"Discriminator loss\"]=dis_loss\n",
        "    history[\"Discriminator accuracy\"]=dis_accuracy\n",
        "\n",
        "    history[\"Generator loss\"]=gen_loss\n",
        "    history[\"Generator accuracy\"]=acc_generator\n",
        "    return history\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3DeECXirOqY"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "x_train=x_train.reshape((x_train.shape[0],x_train.shape[1],x_train.shape[2],1))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch5e6fc1rWy5"
      },
      "source": [
        "GAN=DCGAN(x_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KssrHxCMEBgv"
      },
      "source": [
        "history=GAN.train(500,128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hHeN_PP4eFC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}