{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_training_on_full_data_CIFAR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOQGdJQOBxdn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIlhJ2rtI-qh"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization,Dense,Conv2D,Reshape,Conv2DTranspose,ReLU,LeakyReLU,Flatten,Activation,Dropout\n",
        "from tensorflow.keras.optimizers import  Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3ckRjdbJwVL"
      },
      "source": [
        "class DCGAN:\n",
        "  def __init__(self,training_images):\n",
        "\n",
        "    ### Considering channels last\n",
        "\n",
        "    self.no_of_samples=training_images.shape[0]\n",
        "    self.height=training_images.shape[1]\n",
        "    self.width=training_images.shape[2]\n",
        "    self.channels=training_images.shape[3]\n",
        "    self.train_data=(training_images-127.5)/127.5 ### Converting grey scale [0-255] -> [-1,1]\n",
        "    self.shape=(self.height,self.width,self.channels)\n",
        "    self.noise_size=100\n",
        "    self.Discriminator=None\n",
        "    self.Generator=None\n",
        "    self.disc_optimizer=Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "    self.gen_optimizer=Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "    self.loss=BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "\n",
        "  def get_generator(self):\n",
        "\n",
        "    ### Generator definition\n",
        "\n",
        "    w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    Generator= Sequential()\n",
        "\n",
        "    ### Input layer: takes in input a random noise of 100 points distributed in a random distribution \n",
        "    Generator.add(Dense(int(self.height/4)*int(self.width/4)*256,use_bias=False, input_shape=(self.noise_size,),kernel_initializer=w_init))\n",
        "    Generator.add(BatchNormalization())\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Reshaping layer to reshape in image dimension\n",
        "    Generator.add(Reshape((int(self.height/4),int(self.width/4),256)))\n",
        "\n",
        "    ### Upconv layer 1  ## The size remains constant (7 x 7 x 128)\n",
        "    Generator.add(Conv2DTranspose(128, (5,5), strides=(1,1),padding=\"same\",kernel_initializer=w_init,use_bias=False))\n",
        "    Generator.add(BatchNormalization())\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Upconv layer 2  ## The size upsamples by 2 (14 x 14 x 128)\n",
        "    Generator.add(Conv2DTranspose(64, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Generator.add(BatchNormalization())\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Upconv layer 3  ## The size upsamples by 2 (28 x 28 x 1)\n",
        "    Generator.add(Conv2DTranspose(self.channels, (5,5), strides=(2,2),padding=\"same\",kernel_initializer=w_init,activation=\"tanh\"))\n",
        "\n",
        "    ### generator output must be of the dimension of the input shape of the image data: (28,28,1)\n",
        "    ### The last layer uses tanh activation in the generator. \n",
        "    return Generator\n",
        "\n",
        "  def get_discriminator(self):    \n",
        "    \n",
        "    ### Discriminator definition\n",
        "    w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    Discriminator= Sequential()\n",
        "\n",
        "    ### Input layer: Conv layer 1\n",
        "    Discriminator.add(Conv2D(64, (5,5), strides=(2,2),input_shape=self.shape,padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Discriminator.add(BatchNormalization())\n",
        "    Discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    ### Conv layer 2\n",
        "    Discriminator.add(Conv2D(128, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Discriminator.add(BatchNormalization())\n",
        "    Discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    ### Flatten\n",
        "    Discriminator.add(Flatten())\n",
        "\n",
        "    ### Prediction layer\n",
        "    Discriminator.add(Dense(1,kernel_initializer=w_init))\n",
        "\n",
        "    # Discriminator compilation\n",
        "\n",
        "    return Discriminator\n",
        "  def get_all_models(self):\n",
        "\n",
        "    self.Discriminator=self.get_discriminator()\n",
        "\n",
        "    self.Generator=self.get_generator()\n",
        "\n",
        "    ### Compiling discriminator\n",
        "\n",
        "    print(\"------------ DISCRIMINATOR------------------\")\n",
        "    print(self.Discriminator.summary())\n",
        "    print(\"------------ GENERATOR------------------\")\n",
        "    print(self.Generator.summary())\n",
        "    \n",
        "    return None\n",
        "  def train_on_batch_disc(self,data,labels):\n",
        "    \n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      output=self.Discriminator(data)\n",
        "      loss_disc=self.loss(labels,output)\n",
        "    \n",
        "    d_grads=disc_tape.gradient(loss_disc,self.Discriminator.trainable_variables)\n",
        "    self.disc_optimizer.apply_gradients(zip(d_grads,self.Discriminator.trainable_variables))\n",
        "\n",
        "    pred_prob=tf.round(tf.nn.sigmoid(self.Discriminator(data,training=False)))\n",
        "    disc_acc=tf.reduce_mean(tf.cast(tf.equal(labels,pred_prob),dtype=tf.float32))\n",
        "    acc=disc_acc.numpy()\n",
        "    return loss_disc,acc\n",
        "  \n",
        "  def train_on_batch_gen(self,data,labels):\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape:\n",
        "\n",
        "      output=self.Discriminator(self.Generator(data,training=False))\n",
        "      loss_gen=self.loss(labels,output)\n",
        "    \n",
        "    g_grads=gen_tape.gradient(loss_gen,self.Generator.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(g_grads,self.Generator.trainable_variables))\n",
        "\n",
        "    pred_prob=tf.round(tf.nn.sigmoid(self.Discriminator(self.Generator(data,training=False),training=False)))\n",
        "    gen_acc=tf.reduce_mean(tf.cast(tf.equal(labels,pred_prob),dtype=tf.float32))\n",
        "    acc=gen_acc.numpy()\n",
        "    return loss_gen,acc\n",
        "\n",
        "  def show_images(self, rows=4, columns=4):\n",
        "\n",
        "    z = tf.random.uniform([rows*columns,self.noise_size])\n",
        "\n",
        "    generated_images = self.Generator.predict(z)\n",
        "\n",
        "    generated_images= (generated_images - (-1))/(1 - (-1))\n",
        "\n",
        "    ### Min-Max scaling to convert pixles from [-1,1] -> [0,1]\n",
        "\n",
        "    ### Plotting\n",
        "    fig, ax = plt.subplots(rows,columns, figsize=(6,6))\n",
        "\n",
        "    for i in range(rows):\n",
        "      for j in range(columns):\n",
        "        ax[i,j].imshow(generated_images[i+j,:,:,:])\n",
        "    plt.show()\n",
        "  \n",
        "  def train(self,epochs,batch_size):\n",
        "\n",
        "    self.get_all_models()  ### Initializing all models\n",
        "\n",
        "    dataset=tf.data.Dataset.from_tensor_slices(self.train_data).shuffle(self.no_of_samples).batch(batch_size,\n",
        "                                                                                                 drop_remainder=True)\n",
        "\n",
        "    gen_loss=[]\n",
        "    dis_loss=[]\n",
        "\n",
        "    dis_accuracy=[]\n",
        "    gen_accuracy=[]\n",
        "    \n",
        "    history={}\n",
        "\n",
        "    no_of_batches=self.no_of_samples/batch_size\n",
        "\n",
        "    real_labels=np.ones((batch_size,1)) ### For the real images, the label: 1\n",
        "    ### The discriminator must predict 1 for the real images\n",
        "\n",
        "    fake_labels=np.zeros((batch_size,1))### For the fake images, generated by the generator, the label: 0\n",
        "    ### The discriminator must predict 0 for the fake images\n",
        "\n",
        "    for epoch in range(epochs):      ### Training epochs\n",
        "      d_loss=0\n",
        "      g_loss=0\n",
        "      d_acc=0\n",
        "      g_acc=0\n",
        "\n",
        "      for samples in dataset:\n",
        "\n",
        "        ########    DISCRIMINATOR TRAINING    #########\n",
        "\n",
        "\n",
        "        z = tf.random.uniform([batch_size,self.noise_size]) ### Produces batch_size number of 100 dimensional noise arrays of uniform distribution\n",
        "        \n",
        "        generated=self.Generator.predict(z)  ### obtaining the generated images\n",
        "\n",
        "        training_set_of_discriminator=np.concatenate((samples,generated))\n",
        "        labels_of_discriminator=np.concatenate((real_labels,fake_labels))\n",
        "\n",
        "        loss_disc,acc_disc=self.train_on_batch_disc(training_set_of_discriminator,labels_of_discriminator)\n",
        "\n",
        "\n",
        "        ########    GENERATOR TRAINING    #########\n",
        "\n",
        "\n",
        "        loss_generator,acc_generator=self.train_on_batch_gen(z,real_labels) ### Discriminator is fixed, so the loss is actually genertors\n",
        "      \n",
        "        d_loss+=loss_disc\n",
        "        g_loss+=loss_generator\n",
        "        d_acc+=acc_disc\n",
        "        g_acc+=acc_generator\n",
        "      \n",
        "      d_loss/=no_of_batches\n",
        "      g_loss/=no_of_batches\n",
        "      d_acc/=no_of_batches\n",
        "      g_acc/=no_of_batches\n",
        "\n",
        "\n",
        "      dis_loss.append(d_loss)\n",
        "      dis_accuracy.append(d_acc)\n",
        "\n",
        "      gen_loss.append(g_loss)\n",
        "      gen_accuracy.append(g_acc)\n",
        "\n",
        "      if epoch%20==0:\n",
        "        #self.show_images()\n",
        "\n",
        "        print(f\"ON EPOCH {epoch} Discriminator Loss: {d_loss}, Discriminator accuracy: {d_acc}, Generator loss: {g_loss}, GAN accuracy: {g_acc}\")\n",
        "  \n",
        "    history[\"Discriminator loss\"]=dis_loss\n",
        "    history[\"Discriminator accuracy\"]=dis_accuracy\n",
        "\n",
        "    history[\"Generator loss\"]=gen_loss\n",
        "    history[\"Generator accuracy\"]=acc_generator\n",
        "    return history\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmVgbp23LSOH"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.cifar10.load_data()\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMKaj1zdLZFz"
      },
      "source": [
        "GAN=DCGAN(x_train)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ecA6lTtLV5s"
      },
      "source": [
        "history=GAN.train(1000,128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQLSt8RALXX9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}