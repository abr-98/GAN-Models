{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Semi Supervised WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoM37WuxrjvC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wwybrbFunat"
      },
      "source": [
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import BatchNormalization,Dense,Conv2D,Reshape,Conv2DTranspose,ReLU,LeakyReLU,Flatten,Activation,Dropout,Input\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtAzkaI3utaf"
      },
      "source": [
        "class sWGAN:\n",
        "  def __init__(self,training_images,targets):\n",
        "    self.no_of_samples=training_images.shape[0]\n",
        "    self.height=training_images.shape[1]\n",
        "    self.width=training_images.shape[2]\n",
        "    self.channels=training_images.shape[3]\n",
        "    self.train_data=(training_images-127.5)/127.5 ### Converting grey scale [0-255] -> [-1,1]\n",
        "    self.shape=(self.height,self.width,self.channels)\n",
        "    self.no_of_classes=len(np.unique(targets,return_counts=False))+1\n",
        "    self.targets=to_categorical(targets,self.no_of_classes)\n",
        "    self.target_shape=self.targets.shape\n",
        "    self.noise_size=100\n",
        "    self.Generator= None\n",
        "    self.Critic_Classifier= None\n",
        "    self.clip_value=0.01\n",
        "    self.cross_entropy=CategoricalCrossentropy(from_logits=False)\n",
        "    self.n_critic= 5  \n",
        "\n",
        "    self.gen_optimizer=RMSprop(0.00005)\n",
        "    self.critic_optimizer=RMSprop(0.00005)\n",
        "\n",
        "    self.current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    self.log_dir = 'logs/' + self.current_time\n",
        "    self.summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
        "\n",
        "  def get_generator(self):\n",
        "    \n",
        "    ### Generator definition\n",
        "\n",
        "    w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    Generator= Sequential()\n",
        "\n",
        "    ### Input layer: takes in input a random noise of 100 points distributed in a random distribution \n",
        "    Generator.add(Dense(int(self.height/4)*int(self.width/4)*256,use_bias=False, input_shape=(self.noise_size,),kernel_initializer=w_init))\n",
        "    Generator.add(BatchNormalization(momentum=0.8))\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Reshaping layer to reshape in image dimension\n",
        "    Generator.add(Reshape((int(self.height/4),int(self.width/4),256)))\n",
        "\n",
        "    ### Upconv layer 1  ## The size remains constant (7 x 7 x 128)\n",
        "    Generator.add(Conv2DTranspose(128, (5,5), strides=(1,1),padding=\"same\",kernel_initializer=w_init,use_bias=False))\n",
        "    Generator.add(BatchNormalization(momentum=0.8))\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Upconv layer 2  ## The size upsamples by 2 (14 x 14 x 128)\n",
        "    Generator.add(Conv2DTranspose(64, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init))\n",
        "    Generator.add(BatchNormalization(momentum=0.8))\n",
        "    Generator.add(ReLU())\n",
        "\n",
        "    ### Upconv layer 3  ## The size upsamples by 2 (28 x 28 x 1)\n",
        "    Generator.add(Conv2DTranspose(self.channels, (5,5), strides=(2,2),padding=\"same\",kernel_initializer=w_init,activation=\"tanh\"))\n",
        "\n",
        "    return Generator\n",
        "\n",
        "  def get_critic_classifier(self):\n",
        "\n",
        "    w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    input_tensor=Input((self.shape))\n",
        "\n",
        "    conv1=Conv2D(64, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init,name=\"conv1\")(input_tensor)\n",
        "    batch_1=BatchNormalization(momentum=0.8,name=\"bn1\")(conv1)\n",
        "    activation_1=LeakyReLU(0.2,name=\"ac1\")(batch_1)\n",
        "\n",
        "    conv2=Conv2D(128, (5,5), strides=(2,2),padding=\"same\",use_bias=False,kernel_initializer=w_init,name=\"conv2\")(activation_1)\n",
        "    batch_2=BatchNormalization(momentum=0.8,name=\"bn2\")(conv2)\n",
        "    activation_2=LeakyReLU(0.2,name=\"ac2\")(batch_2)\n",
        "\n",
        "    flattened=Flatten()(activation_2)\n",
        "    \n",
        "    #### Classifier head\n",
        "\n",
        "    dense_1_cl=Dense(256,kernel_initializer=w_init,name=\"cld1\")(flattened)\n",
        "    dense_1_cl=BatchNormalization(momentum=0.8,name=\"clbn1\")(dense_1_cl)\n",
        "    dense_1_cl=ReLU(name=\"clac1\")(dense_1_cl)\n",
        "    dense_1_cl=Dropout(0.2,name=\"cldr1\")(dense_1_cl)\n",
        "\n",
        "    dense_2_cl=Dense(128,kernel_initializer=w_init,name=\"cld2\")(dense_1_cl)\n",
        "    dense_2_cl=BatchNormalization(momentum=0.8,name=\"clbn2\")(dense_2_cl)\n",
        "    dense_2_cl=ReLU(name=\"clac2\")(dense_2_cl)\n",
        "    dense_2_cl=Dropout(0.2,name=\"cldr2\")(dense_2_cl)\n",
        "\n",
        "\n",
        "    output_cl=Dense(self.no_of_classes,kernel_initializer=w_init,activation=\"softmax\",name=\"cl_out\")(dense_2_cl)\n",
        "\n",
        "    #### Critic head\n",
        "\n",
        "    dense_1_cr=Dense(128,kernel_initializer=w_init,use_bias=False,name=\"crd1\")(flattened)\n",
        "    dense_1_cr=BatchNormalization(momentum=0.8,name=\"crbn1\")(dense_1_cr)\n",
        "    dense_1_cr=ReLU(name=\"crac1\")(dense_1_cr)\n",
        "\n",
        "    output_cr=Dense(1,kernel_initializer=w_init,name=\"cr_out\")(dense_1_cr)\n",
        "\n",
        "\n",
        "    model=Model(inputs=[input_tensor],outputs=[output_cr,output_cl])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  \n",
        "  def get_all_models(self):\n",
        "\n",
        "    self.Generator=self.get_generator()\n",
        "    self.Critic_Classifier=self.get_critic_classifier()\n",
        "\n",
        "    print(\"############# Generator ###############\")\n",
        "\n",
        "    print(self.Generator.summary())\n",
        "\n",
        "    print(\"############# Critic-Classifier ###############\")\n",
        "\n",
        "    print(self.Critic_Classifier.summary())\n",
        "\n",
        "    return None\n",
        "  \n",
        "  def train_on_batch_disc(self,data,fake,label_data,label_fake):\n",
        "    \n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      critic_out_real,class_out_real=self.Critic_Classifier(data)\n",
        "      critic_out_fake,class_out_fake=self.Critic_Classifier(fake)\n",
        "      loss_class_real=self.cross_entropy(label_data,class_out_real)\n",
        "      loss_class_fake=self.cross_entropy(label_fake,class_out_fake)\n",
        "      loss_class=(loss_class_real+loss_class_fake)/2\n",
        "      loss_critic=tf.reduce_mean(critic_out_fake)-tf.reduce_mean(critic_out_real)\n",
        "      loss_total=loss_critic+loss_class\n",
        "    \n",
        "    \n",
        "    d_grads=disc_tape.gradient(loss_total,self.Critic_Classifier.trainable_variables)\n",
        "    self.critic_optimizer.apply_gradients(zip(d_grads,self.Critic_Classifier.trainable_variables))\n",
        "\n",
        "    _,pred_real=self.Critic_Classifier(data,training=False)\n",
        "    _,pred_fake=self.Critic_Classifier(fake,training=False)\n",
        "    pred_prob_real=tf.round(pred_real)\n",
        "    pred_prob_fake=tf.round(pred_fake)\n",
        "    disc_acc_real=tf.reduce_mean(tf.cast(tf.equal(label_data,pred_prob_real),dtype=tf.float32))\n",
        "    disc_acc_fake=tf.reduce_mean(tf.cast(tf.equal(label_fake,pred_prob_fake),dtype=tf.float32))\n",
        "    acc_real=disc_acc_real.numpy()\n",
        "    acc_fake=disc_acc_fake.numpy()\n",
        "    acc=(acc_real+acc_fake)/2\n",
        "\n",
        "    return loss_critic,loss_class,loss_total,acc\n",
        "\n",
        "  def train_on_batch_generator(self,noise):\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      output_cr_fake,output_cl_fake=self.Critic_Classifier(self.Generator(noise,training=False),training=False)\n",
        "      loss_gen=-tf.reduce_mean(output_cr_fake)\n",
        "\n",
        "    g_grads=gen_tape.gradient(loss_gen,self.Generator.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(g_grads,self.Generator.trainable_variables))\n",
        "\n",
        "    return loss_gen\n",
        "\n",
        "  def show_images(self, rows=4, columns=4):\n",
        "\n",
        "    z = tf.random.uniform( minval=-1, maxval=1,shape=[rows*columns,self.noise_size]) \n",
        "\n",
        "    generated_images = self.Generator.predict(z)\n",
        "    output_cr,output_cl=self.Critic_Classifier(generated_images,training=False)\n",
        "\n",
        "    generated_images= (generated_images - (-1))/(1 - (-1))\n",
        "    _,labels=self.Critic_Classifier(generated_images)\n",
        "\n",
        "    ### Min-Max scaling to convert pixles from [-1,1] -> [0,1]\n",
        "    figure = plt.figure(figsize=(10,10))\n",
        "    ### Plotting\n",
        "    for i in range(rows*columns):\n",
        "      label=np.argmax(labels[i])\n",
        "      plt.subplot(rows, columns, i+1)\n",
        "      plt.xlabel(label)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.axis('off')\n",
        "      plt.grid(False)\n",
        "      plt.imshow(generated_images[i,:,:,0], cmap=plt.cm.binary) \n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    return image\n",
        "  \n",
        "\n",
        "  def train(self,epochs,batch_size):\n",
        "\n",
        "    self.get_all_models()  ### Initializing all models     \n",
        "    dataset=tf.data.Dataset.from_tensor_slices((self.train_data,self.targets)).shuffle(self.no_of_samples).batch(batch_size,drop_remainder=True)                                                                           \n",
        "    gen_loss=[]\n",
        "    dis_loss=[]\n",
        "    t_loss=[]\n",
        "    classifier_loss=[]\n",
        "    classifier_acc=[]\n",
        "\n",
        "    history={}\n",
        "\n",
        "\n",
        "    no_of_batches=self.no_of_samples/batch_size\n",
        "    \n",
        "    for epoch in range(epochs):      ### Training epoch\n",
        "\n",
        "      iterator=dataset.as_numpy_iterator()\n",
        "\n",
        "    #### Critic Training\n",
        "\n",
        "      d_loss=0\n",
        "      total_loss=0\n",
        "      g_loss=0 \n",
        "      c_loss=0\n",
        "      c_acc=0\n",
        "\n",
        "      for _ in range(50):\n",
        "\n",
        "        d_loss_n_crit=0\n",
        "        c_loss_n_crit=0\n",
        "        c_acc_n_crit=0\n",
        "        total_loss_n_crit=0\n",
        "\n",
        "        for _ in range(self.n_critic):\n",
        "\n",
        "          z = tf.random.uniform( minval=-1, maxval=1,shape=[batch_size,self.noise_size])\n",
        "          generated=self.Generator.predict(z)\n",
        "          fake_labels=to_categorical(np.full((batch_size, 1), self.no_of_classes-1), num_classes=self.no_of_classes)\n",
        "          #print(fake_labels)\n",
        "          (samples,labels)=iterator.next()\n",
        "  \n",
        "\n",
        "          loss_critic,loss_class,loss_total,acc_real=self.train_on_batch_disc(samples,generated,labels,fake_labels)\n",
        "         \n",
        "          for layer in self.Critic_Classifier.layers:\n",
        "            weights_crit = layer.get_weights()\n",
        "            if layer.name not in [\"cld1\",\"clbn1\",\"clac1\",\"cldr1\",\"cld2\",\"clbn2\",\"clac2\",\"cldr2\",\"cl_out\"]:\n",
        "              weights_mod_crit = [np.clip(w, -self.clip_value, self.clip_value) for w in weights_crit]\n",
        "            else:\n",
        "              weights_mod_crit=weights_crit\n",
        "            layer.set_weights(weights_mod_crit)\n",
        "\n",
        "\n",
        "\n",
        "          d_loss_n_crit+=loss_critic\n",
        "          c_loss_n_crit+=loss_class\n",
        "          c_acc_n_crit+=acc_real\n",
        "          total_loss_n_crit+=loss_total\n",
        "        \n",
        "        d_loss_n_crit/=self.n_critic\n",
        "        c_loss_n_crit/=self.n_critic\n",
        "        c_acc_n_crit/=self.n_critic\n",
        "        total_loss_n_crit/=self.n_critic\n",
        "        d_loss/=self.n_critic\n",
        "        \n",
        "        z = tf.random.uniform( minval=-1, maxval=1,shape=[batch_size,self.noise_size]) \n",
        "        loss_gen=self.train_on_batch_generator(z)\n",
        "        \n",
        "        d_loss+=d_loss_n_crit\n",
        "        c_loss+=c_loss_n_crit\n",
        "        c_acc+=c_acc_n_crit\n",
        "        total_loss+=total_loss_n_crit\n",
        "        g_loss+=loss_gen\n",
        "\n",
        "        \n",
        "      \n",
        "      d_loss/=50\n",
        "      g_loss/=50\n",
        "      total_loss/=50\n",
        "      c_loss/=50\n",
        "      c_acc/=50\n",
        "      dis_loss.append(d_loss)\n",
        "      gen_loss.append(g_loss)\n",
        "      t_loss.append(total_loss)\n",
        "      classifier_loss.append(c_loss)\n",
        "      classifier_acc.append(c_acc)\n",
        "\n",
        "      print(f\"ON EPOCH {epoch} Critic Loss: {d_loss}, Generator loss: {g_loss}, classifier loss: {c_loss}, total_loss: {total_loss}, classifier accuracy: {c_acc}\")\n",
        "      with self.summary_writer.as_default():\n",
        "        tf.summary.scalar('loss/Generator', g_loss, step=epoch)\n",
        "        tf.summary.scalar('loss/Critic', d_loss, step=epoch)\n",
        "        tf.summary.scalar('loss/Classifier', c_loss, step=epoch)\n",
        "        tf.summary.scalar('loss/Total', total_loss, step=epoch)\n",
        "        tf.summary.scalar('acc/classifier', c_acc, step=epoch)\n",
        "\n",
        "      \n",
        "      if epoch%50==0:\n",
        "        figs=self.show_images()\n",
        "        tf.summary.image('gen_images', figs, step=epoch)\n",
        "\n",
        "\n",
        "    history[\"Critic loss\"]=dis_loss\n",
        "\n",
        "    history[\"Generator loss\"]=gen_loss\n",
        "    history[\"Total Loss\"]=t_loss\n",
        "    history[\"Classifier loss\"]=classifier_loss\n",
        "    history[\"Classifier accuracy\"]=classifier_acc\n",
        "    return history\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQP-DK1m3dHt"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "x_train=x_train.reshape((x_train.shape[0],x_train.shape[1],x_train.shape[2],1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKzBgVWK4TLd"
      },
      "source": [
        "GAN=sWGAN(x_train,y_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "646SwondzCyN",
        "outputId": "bb510315-b36f-4fe6-ebac-5ecfdfb04102"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 272), started 0:48:07 ago. (Use '!kill 272' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yceuig4J33pt"
      },
      "source": [
        "history=GAN.train(2000,64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrCPn9ts6Ot5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}